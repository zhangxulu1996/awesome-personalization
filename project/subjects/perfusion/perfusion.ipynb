{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huwentao/anaconda3/envs/perfusion/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from scripts.helpers import chunk, load_model_from_config\n",
    "from scripts.helpers import sample as advanced_sample\n",
    "\n",
    "\n",
    "def perfusion_t2i(prompt_templates,\n",
    "                  outdir,\n",
    "                  personalized_ckpt,\n",
    "                  step=50,\n",
    "                  ddim_eta=0.0,\n",
    "                  n_iter=1,\n",
    "                  H=512,\n",
    "                  W=512,\n",
    "                  C=4,\n",
    "                  f=8,\n",
    "                  n_samples=4,\n",
    "                  scale=6,\n",
    "                  beta=0.7,\n",
    "                  tau=0.15,\n",
    "                  config=\"configs/perfusion_inference.yaml\",\n",
    "                  ckpt=\"./ckpt/v1-5-pruned-emaonly.ckpt\",\n",
    "                  seed=42,\n",
    "                  precision=\"autocast\", # choices=[\"full\", \"autocast\"],\n",
    "                  global_locking=False\n",
    "                ):\n",
    "\n",
    "    assert torch.cuda.is_available()\n",
    "    device = \"cuda\"\n",
    "    batch_size = n_samples\n",
    "    shape = [C, H // f, W // f]\n",
    "\n",
    "    seed_everything(seed)\n",
    "\n",
    "    config = OmegaConf.load(f\"{config}\")\n",
    "    personalized_ckpts = personalized_ckpt.split(',')\n",
    "    n_concepts = len(personalized_ckpts)\n",
    "    if n_concepts > 1:\n",
    "        config.model.target = 'perfusion.perfusion.MultiConceptsPerfusion'\n",
    "        config.model.params.n_concepts = n_concepts\n",
    "    else:\n",
    "        personalized_ckpts = personalized_ckpts[0]\n",
    "\n",
    "    config.model.params.beta = beta\n",
    "    config.model.params.tau = tau\n",
    "    model = load_model_from_config(config, ckpt, personalized_ckpts)\n",
    "    model = model.to(device)\n",
    "\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "    sample = lambda c, uc: (\n",
    "        sampler.sample(\n",
    "            S=step,\n",
    "            conditioning=c,\n",
    "            batch_size=batch_size,\n",
    "            shape=shape,\n",
    "            verbose=False,\n",
    "            unconditional_guidance_scale=scale,\n",
    "            unconditional_conditioning=uc,\n",
    "            eta=ddim_eta,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = outdir\n",
    "\n",
    "    for prompt in prompt_templates:\n",
    "\n",
    "        print(f\"**Prompt**: {prompt}\")\n",
    "\n",
    "        assert prompt is not None\n",
    "        data = [batch_size * [prompt]]\n",
    "\n",
    "        # prompts with placeholder word\n",
    "        placeholders = list(model.embedding_manager.string_to_token_dict.keys())\n",
    "        superclasses = model.embedding_manager.initializer_words\n",
    "        data_concept = list()\n",
    "        data_superclass = list()\n",
    "        for i in range(len(data)):\n",
    "            data_concept.append(list())\n",
    "            data_superclass.append(list())\n",
    "            for j in range(len(data[i])):\n",
    "                prompt_concept, prompt_superclass = data[i][j], data[i][j]\n",
    "                for concept_i in range(n_concepts):\n",
    "                    target = f'{{{concept_i + 1}}}' if n_concepts > 1 else '{}'\n",
    "                    prompt_concept = prompt_concept.replace(target, placeholders[concept_i])\n",
    "                    prompt_superclass = prompt_superclass.replace(target, superclasses[concept_i])\n",
    "                data_concept[i].append(prompt_concept)\n",
    "                data_superclass[i].append(prompt_superclass)\n",
    "\n",
    "        sample_path = os.path.join(outpath, prompt.replace(\"{}\", \"_\"))\n",
    "\n",
    "        os.makedirs(sample_path, exist_ok=True)\n",
    "        base_count = len(os.listdir(sample_path))\n",
    "\n",
    "        precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(device):\n",
    "                with model.ema_scope():\n",
    "                    for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                        for data_i in tqdm(range(len(data_concept)), desc=\"data\"):\n",
    "                            prompts = data_concept[data_i]\n",
    "                            prompts_superclass = data_superclass[data_i] if global_locking else None\n",
    "\n",
    "                            uc = None\n",
    "                            if scale != 1.0:\n",
    "                                encoding_uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                                uc = dict(c_crossattn=encoding_uc,\n",
    "                                        c_super=encoding_uc if global_locking else None)\n",
    "                            if isinstance(prompts, tuple):\n",
    "                                prompts = list(prompts)\n",
    "                            encoding = model.cond_stage_model.encode(prompts, embedding_manager=model.embedding_manager)\n",
    "                            encoding_superclass = model.get_learned_conditioning(prompts_superclass) if global_locking else None\n",
    "                            c = dict(c_crossattn=encoding, c_super=encoding_superclass)\n",
    "\n",
    "                            z_samples = sample(c, uc)\n",
    "                            x_samples = model.decode_first_stage(z_samples)\n",
    "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                            for x_sample in x_samples:\n",
    "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                    os.path.join(sample_path, f\"{base_count:04d}.jpg\"))\n",
    "                                base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import sys\n",
    "utils_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "from utils.dataset_info import get_subjects_prompts_info\n",
    "\n",
    "\n",
    "# Single Subject Generation\n",
    "single_subject = []                            # \"backpack\"\n",
    "# Single Prompt Generation\n",
    "single_prompt = []                            # e.g. [\"a {0} {1} near the pool\"]\n",
    "\n",
    "num_generation = 4\n",
    "\n",
    "\n",
    "output_path = \"../../outputs/subjects/perfusion\"\n",
    "logs_path = \"../../logs/subjects/perfusion/\"\n",
    "subjects = os.listdir(logs_path)\n",
    "dataset_info_path = \"../../pcs_dataset/info.json\"\n",
    "\n",
    "prompts_info = get_subjects_prompts_info(dataset_info_path)\n",
    "\n",
    "if len(single_subject):\n",
    "    subjects = single_subject\n",
    "\n",
    "for subject in subjects:\n",
    "        \n",
    "    print(f\"***** Subject: {subject} *****\")\n",
    "\n",
    "    outdir = os.path.join(output_path, subject)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    if len(single_prompt):\n",
    "        prompts = single_prompt\n",
    "    else:\n",
    "        prompts = prompts_info[subject][\"prompts\"]\n",
    "        prompts.append(\"a photo of a {0} {1}\")\n",
    "\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        prompts[idx] = prompt.replace(\"{0} {1}\", \"{}\")\n",
    "    \n",
    "    \n",
    "    personalized_ckpt = os.path.join(logs_path, subject, \"models/last.ckpt\")\n",
    "    \n",
    "    perfusion_t2i(prompts, outdir, personalized_ckpt)\n",
    "    \n",
    "    print(f\"Finished perfusion in subject: {subject}!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the similarity for each sample\n",
    "Calculate img to img similarity and text to img similarity by CLIP Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "utils_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "import numpy as np\n",
    "from utils.clip_eval import evaluate_i2i, evaluate_t2i\n",
    "from utils.dataset_info import get_subjects_prompts_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, np.generic):\n",
    "        return data.item()\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: convert_to_native(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_native(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "def add_evaluation(file_path, new_data):\n",
    "    # check whether the file exist\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(convert_to_native(new_data), file, indent=4)\n",
    "    else:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        data.update(convert_to_native(new_data))\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_path = \"../../outputs/subjects/perfusion\"\n",
    "eval_res_path = \"../../eval_results/subjects/perfusion\"\n",
    "dataset_path = \"../../pcs_dataset/subjects\"\n",
    "dataset_info_path = \"../../pcs_dataset/info.json\"\n",
    "os.makedirs(eval_res_path, exist_ok=True)\n",
    "\n",
    "subjects_list = os.listdir(outputs_path)\n",
    "prompts_info = get_subjects_prompts_info(dataset_info_path)\n",
    "\n",
    "for subject in subjects_list:\n",
    "    evaluation_res = dict()\n",
    "    print(f\"***** Subject: {subject} *****\")\n",
    "\n",
    "    res_for_each_subject =dict()\n",
    "\n",
    "    for prompt in os.listdir(os.path.join(outputs_path, subject)):\n",
    "\n",
    "        prompt_eval = prompt.replace(\"_\", prompts_info[subject][\"class\"])\n",
    "\n",
    "        print(f\"**Prompt**: {prompt_eval}\")\n",
    "\n",
    "        res_for_each_prompt =dict()\n",
    "\n",
    "        for generate_img_name in os.listdir(os.path.join(outputs_path, subject, prompt)):\n",
    "            generate_img_path = os.path.join(outputs_path, subject, prompt, generate_img_name)\n",
    "            res_for_each_prompt[generate_img_name] = [evaluate_i2i(generate_img_path, os.path.join(dataset_path, subject)), evaluate_t2i(generate_img_path, prompt_eval)]\n",
    "        \n",
    "        res_for_each_subject[prompt] = res_for_each_prompt\n",
    "        print(res_for_each_prompt)\n",
    "\n",
    "    evaluation_res[subject] = res_for_each_subject\n",
    "\n",
    "    add_evaluation(os.path.join(eval_res_path, \"evaluation_results.json\"), evaluation_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Similarity:  0.6911645406771372 \n",
      "Text Similarity: 0.2725124368998157\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "eval_res_path = \"../../eval_results/subjects/perfusion/evaluation_results.json\"\n",
    "\n",
    "with open(eval_res_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "img_sim = 0.0\n",
    "text_sim = 0.0\n",
    "cnt = 0\n",
    "\n",
    "for subject in data:\n",
    "    for prompt in data[subject]:\n",
    "        for sample in data[subject][prompt]:\n",
    "            img_sim = img_sim + data[subject][prompt][sample][0]\n",
    "            text_sim = text_sim + data[subject][prompt][sample][1]\n",
    "            cnt = cnt + 1\n",
    "print(\"Image Similarity: \", img_sim/cnt, \"\\nText Similarity:\", text_sim/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
